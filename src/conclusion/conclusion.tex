\chapter{Conclusion}
\label{conclusion}
In the final chapter, we summarize this thesis's contributions, discuss the implications of the obtained results, and state possible research directions for future work.

\section{Revisiting Research Questions}
In this section, we reflect on the high-level research questions defined in the introduction and discuss the implications of the obtained results for the RQs.

\begin{description}
    \item[\RQ{1}] How effective is call graph pruning for security-focused applications?
\end{description}

The implications of both machine learning-based and non-learning-based call graph pruning techniques offer distinctive advantages and disadvantages, particularly in the context of security-focused applications. Each approach addresses different aspects of the requirements for practical static analysis, providing a multi-faceted perspective on enhancing the reliability and efficiency of security tools.

Machine learning-based call graph pruning, as demonstrated in Chapter~\ref{ch:effect_ml_cg_pruning}, can significantly improve the precision of static call graphs by approximately 25\%. This is a crucial refinement for reducing false positives and enhancing the reliability of downstream analyses. However, this method also introduces a trade-off with reduced recall or soundness, potentially leading to the omission of critical vulnerabilities due to pruned legitimate edges. Therefore, while ML-based pruning shows promise for speeding up security analyses through enhanced precision, it necessitates careful calibration of the balance between precision and recall, using the proposed conservative strategies, to avoid compromising soundness in vulnerability detection.

Conversely, the non-learning-based approach, exemplified by the \tool{OriginPruner} technique (presented in Chapter~\ref{ch:origin_pruner}), leverages domain knowledge and the origin methods to guide pruning decisions. This technique significantly reduces the size of call graphs without the need for extensive training data and at lower computational costs than ML-based methods. By focusing on the origin methods within the class hierarchy and localness analysis, \tool{OriginPruner} maintains the integrity of security analyses and ensures that no crucial edges are mistakenly omitted. This approach is particularly beneficial for security applications requiring rapid and accurate assessments, as it simplifies the call graphs while maintaining soundness, i.e., preserving the essential paths for vulnerability analysis.

The contrasting approaches highlight a different perspective to call graph pruning: the balance between using advanced machine learning techniques that require significant computational resources and training versus employing static, domain-knowledge-based methods that are less resource-intensive but might offer less flexibility in handling dynamic and complex software behaviors. Future research and practical applications in security-focused software analysis will likely benefit from a hybrid approach that combines the precision enhancement of ML-based pruning with the efficiency and reliability of non-learning-based methods. This combined approach could mitigate the limitations of each method and harness its strengths to improve the scalability and accuracy of security tools in real-world scenarios.

\begin{description}
\item[\RQ{2}] How does the call graph-based approach aid in reducing false positives in the vulnerability propagation analysis?
\end{description}

In Chapter~\ref{ch:effect_trans_gran}, we examined the effectiveness of a call graph-based approach in reducing false positives during vulnerability propagation analysis in software ecosystems, specifically within the Maven ecosystem. The study leverages empirical methods to demonstrate how incorporating a fine-grained analysis using call graphs significantly enhances the accuracy of identifying genuinely vulnerable packages by distinguishing between direct and transitive dependencies at different levels of granularity.

One key implication of the results is the substantial reduction in false positives when call graph-based analysis is applied. Traditional methods that evaluate vulnerability based on package dependencies alone tend to overestimate the security risks by marking the whole application with a vulnerable dependency as at risk. However, the findings show that only a few packages have executable paths that reach the vulnerable code within their dependencies. This distinction is critical because it implies that many software systems may not be as vulnerable as previously thought due to the lack of invocation of the insecure code. Thus, the call graph approach not only refines the accuracy of vulnerability assessments but also prioritizes allocating resources to mitigate critical security threats.

Moreover, the findings highlight the impact of granularity in the analysis of vulnerability propagation. By examining vulnerabilities at both the package and method levels, the study provides insights into how different levels of analysis can lead to vastly different results in the context of security risk assessment. For instance, while a package-level analysis identifies many potentially vulnerable packages, a method-level analysis using call graphs often reveals a much smaller subset where the vulnerable code is actively executed. This granular approach helps focus efforts on genuinely critical issues that require immediate attention, thus optimizing the effectiveness of security measures.

Another crucial aspect discussed in Chapter~\ref{ch:effect_trans_gran} is the concept of dependency depth. The study explores how limiting the depth of analysis to direct dependencies only (ignoring deeper transitive dependencies) can significantly reduce the computational burden of the analysis while still capturing a majority of the vulnerabilities that would actually impact the security of the application. This approach suggests a strategic compromise between depth of analysis and resource usage, which is particularly valuable for large-scale systems where extensive dependency chains are common.

In summary, Chapter~\ref{ch:effect_trans_gran} provides compelling evidence that call graph-based analysis is a more precise tool for vulnerability propagation analysis in software ecosystems. This method improves the accuracy of identifying vulnerable packages and helps efficiently prioritize security efforts. The insights from this study could guide future research and practices in software security, particularly in enhancing the tools and methodologies for vulnerability analysis in increasingly complex software environments.

\begin{description}
\item[\RQ{3}] How effective is machine learning in inferring type annotations for Python?
\end{description}

The results presented in Chapter~\ref{ch:t4py} highlight significant advancements in machine learning-based type inference for Python, showing a notable increase in Mean Reciprocal Rank over previous models like Typilus~\cite{allamanis2020typilus} and TypeWriter~\cite{pradel2019typewriter}. This improvement underscores the effectiveness of \tool{Type4Py}'s deep similarity learning approach, which better discriminates between similar and dissimilar types, thus enhancing the accuracy of type predictions. Using a type-checked dataset further adds to the robustness of the model, ensuring that the training and evaluation are based on sound type annotations, which mitigates the risk of learning from potentially incorrect data.

Given that \tool{Type4Py} achieves a higher MRR, particularly in its Top-1 suggestions, this suggests a significant step forward in practical usability. Developers are more likely to adopt a tool that consistently provides accurate suggestions at the top of its output list, as this reduces the effort required to select the correct type annotation manually. This feature directly translates into increased productivity and reduces the potential for errors in code, especially in dynamically typed languages like Python, where such errors are common.

Moreover, software maintainability can be significantly improved through accurate type inference. Type annotations make the code more understandable and easier to navigate, especially for new developers joining a project or revisiting old codebases. The ability of \tool{Type4Py} to retrofit accurate types into existing, untyped, or partially typed codebases can transform legacy Python code into more maintainable and modern code practices, aligning with Python's gradual typing philosophy introduced by PEP 484.

The ability of \tool{Type4Py} to provide highly accurate type suggestions indirectly affects developer productivity. Since the tool can reliably suggest the correct type annotation on the first try, developers spend less time typing and more time on actual problem-solving. This reduction in cognitive load can lead to faster development cycles, quicker feature rollouts, and more time allocated to optimizing code and implementing robust features. Furthermore, integrating such a tool within IDEs, as demonstrated by the Visual Studio Code extension, brings these benefits directly into the developer's environment, offering immediate and accessible benefits to their workflow.

Applying machine learning models like \tool{Type4Py} to infer type information pushes the boundary of what is possible with advanced programming practices in dynamically typed languages. It potentially opens the door to more sophisticated static analysis tools that were traditionally more effective in statically typed languages. For instance, more accurate type inference can enhance refactoring tools, code completion features, and sophisticated code analysis tools that can perform more in-depth checks and optimizations based on predicted types.

\section{Discussion}
In this section, based on the work presented in this thesis, we discuss 
the implications of the obtained results and research directions for future work.

\paragraph{Gathering high-quality data to train ML models is expensive} In Chapter~\ref{ch:effect_ml_cg_pruning} and Chapter~\ref{ch:t4py}, we applied deep learning techniques to tackle call graph pruning and type inference tasks, respectively. Our proposed model, \tool{Type4Py} outperformed state-of-the-art approaches at the time, namely, \tool{Typilus}~\cite{allamanis2020typilus} and \tool{TypeWriter}~\cite{pradel2019typewriter}. Despite the promising results, the proposed techniques must be trained or fine-tuned for the task at hand. Also, preparing ground truth is a daunting task per se, i.e., finding Python projects with sound type annotations or Java projects with high test coverage to build dynamic call graphs. Another alternative is to use heuristics and domain knowledge to solve the task, as shown in Chapter~\ref{ch:origin_pruner}. Very recently, (large) code language models have shown commendable performance in software engineering tasks due to their emergent capabilities, i.e., in-context learning~\cite{zheng2023survey}. Given this, using code language models might be a viable direction for future research in software analysis. Recent code language models, such as Codestral~\cite{codestral}, alleviate the need for data thanks to their zero-shot or few-short learning capabilities.

% \paragraph{Call graph pruning with the real-world vulnerabilities at the ecosystem level.} In Chapter~\ref{ch:effect_ml_cg_pruning} and Chapter~\ref{ch:origin_pruner}, we used artificial vulnerabilities to assess the effectiveness of (ML-based) call graph pruning techniques for the vulnerability analysis. To extend the obtained results in those two chapters, future work should investigate the efficacy of CG pruning techniques with real-world vulnerabilities at the ecosystem scale. Specifically, one can add a call graph pruner to our pipeline in Chapter~\ref{ch:effect_trans_gran} and then study its effect on the vulnerability propagation in the Maven ecosystem.

\paragraph{ML-based developer tools may not generalize beyond training data} In this thesis, we evaluated the trained or fine-tuned ML models on a set of unseen samples, namely, the test set, which is relatively small compared to the myriad of open-source software projects on the internet. Although their performance on the test sets was impressive, it is still unclear how an ML-based developer tool, \tool{Type4Py} would perform in real-world scenarios when used on different projects outside the ManyTypes4Py dataset. As described in Chapter~\ref{ch:t4py}, we gathered telemetry data when \tool{Type4Py} was used by developers in the Visual Studio Code extension. Similar to the studies done to evaluate code completion models in IDEs~\cite{izadi2024language, hellendoorn2019code}, future work can study to what extent developers accept the inferred type annotations by \tool{Type4Py} in the IDE. This also gives further insights into the real-world performance of \tool{Type4Py} and cases where it fails to infer a correct type annotation. Also, we generally recommend assessing the real-world performance of ML models trained for code-related tasks, as the ultimate goal is to boost developers' productivity by using these models.

\paragraph{A need for standard and systematic evaluation of ML-based developer tools} In addition to the real-world evaluation of the ML-based developers' tools, a micro-benchmark is needed to be developed that covers different language features. This is essential for assessing the performance of type inference and call graph pruning tools because it provides a controlled environment to measure and compare their effectiveness, providing modern applications that often leverage diverse language features~\cite{peng2021empirical}. In other words, micro-benchmarks can isolate specific language features, allowing for precise evaluation of how well these tools handle various constructs such as generics, lambdas, and reflection. This granularity helps identify inaccuracies in these tools, enabling researchers or practitioners to improve these tools systematically for better accuracy and completeness. Prasad et al.~\cite{venkatesh2023typeevalpy} proposed a micro-benchmark containing 845 test cases across 18 categories for the type inference task. A similar micro-benchmark should also be created to assess the effectiveness of call graph pruning techniques. For instance, one can start assessing the effect of CG pruning against the test suites proposed for CG soundness, known as the Judge benchmark~\cite{reif2019judge}.

\paragraph{Combination of machine learning and static analysis is promising} In this thesis, we showed that machine learning techniques can effectively be applied to tackle software analysis problems such as type inference and call graph pruning. However, recent work~\cite{peng2022static} has shown that a hybrid approach, i.e., the combination of machine learning and static analysis, is promising for inferring type annotation. Similarly, future work can investigate the efficacy of a hybrid call graph pruning approach, i.e., the combination of ML-based call graph pruners with \tool{OriginPruner}. Specifically, one can study how to use an ML-based CG pruner with \tool{OriginPruner} interchangeably or in combination to prune a CG edge.

\paragraph{Adoption of machine learning-based features into software analysis tools} In Chapter~\ref{ch:effect_ml_cg_pruning}, our empirical findings indicate that machine learning-based call graph pruning is an effective strategy for enhancing the precision of call graphs while maintaining their soundness, particularly in security-focused applications. However, similar to the previous work~\cite{le2022autopruner, utture2022striking}, our approach treats pruning as a subsequent step to constructing the call graph, which introduces a minor computational overhead. An alternative approach could involve embedding the ML-based pruning functionality directly within existing static analysis frameworks such as WALA~\cite{fink2012wala} and OPAL~\cite{eichberg2014software}. This feature enables users to prune edges or methods selectively during the call graph construction phase. This integration has the potential to yield more precise and compact call graphs and speed up the construction process. Aside from the pruning feature, one can achieve more accurate method resolution and variable interaction analysis by embedding a type inference component into the call graph construction tool for dynamic languages (e.g., Python). This, in turn, enhances the precision and soundness of the call graphs, making them more useful for tasks such as static code analysis, security vulnerability detection, and program understanding.

\paragraph{ML-based type inference for dynamic languages is still an open problem} 
In 2022, we published the \tool{Type4Py} technique, an ML-based type inference tool for Python. Following this, several research works have extended Type4Py or proposed other techniques to improve type inference for Python and other dynamic programming languages. These newer techniques include, but are not limited to, OppropBERT~\cite{jha2022oppropbert}, HiTyper~\cite{peng2022static}, TypeT5~\cite{wei2022typet5}, DiverseTyper~\cite{jesse2022learning}, Stir~\cite{peng2023statistical}, DeMinify~\cite{li2023deminify}, Tipical~\cite{elkobi2023tipical}, TypeGen~\cite{peng2023generative}, DeepInfer~\cite{zhao2023deepinfer}, PyAnalyzer~\cite{jin2024pyanalyzer}, and LExecutor~\cite{souza2023lexecutor}. Additionally, Type4Py was utilized as a type inference tool to address other intriguing research problems, such as docstring generation for Python~\cite{venkatkrishna2023docgen}, unit test generation~\cite{lukasczyk2022pynguin}, the development of fuzzing techniques~\cite{li2023pyrtfuzz}, and the study of dynamic typing-related practices in Python~\cite{chen2024risky}. Future work should look into local (small) large language models for type inference with retrieval augmented generation (RAG), which can be used inside IDEs. RAG might potentially help with user-defined type annotations in the project, which the LLM has not seen before.

\paragraph{Software ecosystem-level analysis is insightful but expensive to perform} We introduced the FASTEN project in Chapter~\ref{ch:intro}, which influenced how developers manage their projects' dependencies and assess the impact of security vulnerabilities on their projects. In short, In this thesis, we proposed a conservative ML-based call graph pruner and a non-learning-based approach (\tool{OriginPruner}). Since FASTEN operates at the ecosystem level, i.e., millions of projects and more than 10TB of data, these two CG pruning approaches can be a valuable extension to the FASTEN pipeline to make call graphs smaller and speed up vulnerability analysis. Also, this thesis investigated the vulnerability propagation in the Maven ecosystem from the perspective of transitivity and granularity. The obtained results in Chapter~\ref{ch:effect_trans_gran} empirically show the advantage of a fine-grained approach (i.e., call graph-level analysis) to the over-inflated dependency-level approach when analyzing security vulnerabilities in programs. In other words, we showed that FASTEN could be pretty helpful for developers in better assessing the risk of vulnerabilities found in their programs or dependencies used.  

\section{Summary}
% \subsection{Thesis contributions}
In this thesis, we showed that machine learning is promising for solving software analysis tasks like type inference and call graph pruning. Also, we investigated the effectiveness of dependency- and call graph-level vulnerability assessments in the Maven ecosystem. More specifically, this thesis makes the following contributions:

\begin{itemize}
    \item In this thesis, we introduced NYXCorpus, a benchmark dataset for evaluating machine learning-based call graph pruning techniques. By addressing limitations of the previous work, like imbalanced training data and reduced recall, we could implement conservative pruning strategies that improved the precision of call graphs while maintaining practicality for security applications. Our work also demonstrates that pruned call graphs retain high quality, comparable to context-sensitive analyses (1-CFA), but are produced faster and with smaller sizes (69\%), making downstream applications, i.e., vulnerability propagation, faster (up to 3.5 times).

    \item Given that ML models need high-quality data and have lofty inference cost, we developed \tool{OriginPruner}, a novel method leveraging method origin and localness analysis to prune false edges in static call graphs effectively. This approach not only reduces the size of the graphs but does so without compromising the soundness necessary for critical security applications, such as vulnerability propagation analysis. Our results confirm the effectiveness of incorporating domain-specific knowledge into pruning strategies, improving the precision of static program analysis with little computational overhead.

    \item Dependency-level analysis highly inflates the actual number of affected projects by security vulnerabilities. Motivated by this, this thesis investigated the impact of transitivity and granularity on vulnerability propagation through an empirical study in the Maven ecosystem. By shifting from dependency-level to method-level analysis, we provided a more accurate assessment of vulnerabilities, challenging the conventional overestimation of security risks in the previous work. Specifically, we found that less than 1\% of the packages have a reachable call path to vulnerable code in their dependencies. This fine-grained approach suggests a potential for significantly more efficient and accurate vulnerability assessments, which assists software developers in taking required actions to mitigate the vulnerability. 

    \item Retrofitting type annotations to existing codebases can be error-prone and laborious. To alleviate this, we proposed \tool{Type4Py}, a state-of-the-art ML-based type inference technique for Python, a dynamically typed language. By employing a deep similarity learning model, \tool{Type4Py} effectively distinguishes between type annotations in high-dimensional space, significantly improving the inference accuracy compared to the previous work. \tool{Type4Py} achieves an MRR of 77.1\%, which is a significant improvement of 8.1\% and 16.7\% over the state-of-the-art approaches Typilus and TypeWriter, respectively. Alongside its integration into a Visual Studio Code extension, \tool{Type4Py} aids developers by retrofitting type annotations into existing Python codebases, enhancing both productivity and code quality.
\end{itemize}

In conclusion, our research lays the foundation for a new era in software analysis, where machine learning and domain-specific knowledge converge to revolutionize how we develop and maintain software. Our novel and promising approaches not only push the boundaries of what is possible in software analysis but also pave the way for developing more accurate, scalable, and practical tools that will shape the future of software development. We hope this thesis's results inspire other researchers to explore ML-assisted software analysis and investigate how to seamlessly integrate it into developers' workflows, ultimately aiding them at various stages of the software development lifecycle.